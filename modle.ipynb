{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a675ed20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630ebae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c951b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "true_df = pd.read_csv('dataset/True.csv')\n",
    "fake_df = pd.read_csv('dataset/Fake.csv')\n",
    "\n",
    "# Add label column - 0 for real (true), 1 for fake\n",
    "true_df['label'] = 0\n",
    "fake_df['label'] = 1\n",
    "\n",
    "# Combine the datasets\n",
    "df = pd.concat([true_df, fake_df], axis=0)\n",
    "\n",
    "# Shuffle the dataset to mix fake and real news\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(f\"Total number of samples: {len(df)}\")\n",
    "print(f\"Number of real news: {len(df[df['label']==0])}\")\n",
    "print(f\"Number of fake news: {len(df[df['label']==1])}\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check subject distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='subject', hue='label', data=df)\n",
    "plt.title('Distribution of Subjects by Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Check date distribution (if relevant)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    df[df['label']==0]['date'].dt.year.hist(alpha=0.5, bins=30, label='Real')\n",
    "    df[df['label']==1]['date'].dt.year.hist(alpha=0.5, bins=30, label='Fake')\n",
    "    plt.title('Distribution of News by Year')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    clean_text = ' '.join(tokens)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Combine title and text for better context (optional)\n",
    "df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Apply preprocessing to the combined text\n",
    "df['clean_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Check the results\n",
    "print(\"Original text example:\\n\", df['text'].iloc[0][:200], \"...\")\n",
    "print(\"\\nCleaned text example:\\n\", df['clean_text'].iloc[0][:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a87c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings after cleaning\n",
    "empty_text = df[df['clean_text'].str.len() == 0]\n",
    "print(f\"Number of empty texts after cleaning: {len(empty_text)}\")\n",
    "\n",
    "# Remove rows with empty text if any\n",
    "df = df[df['clean_text'].str.len() > 0]\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['clean_text'])\n",
    "print(f\"Number of duplicate texts: {duplicates.sum()}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "df = df[~duplicates]\n",
    "\n",
    "# Reset index after cleaning\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "final_df = df[['clean_text', 'label']].copy()\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nFinal class distribution:\")\n",
    "print(final_df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Save cleaned dataset (optional)\n",
    "final_df.to_csv('dataset/cleaned_news.csv', index=False)\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
